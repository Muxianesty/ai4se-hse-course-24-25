# Практическое задание на тему "Классификация комментариев на ревью"

Выполнил: Литвинов Михаил Юрьевич, группа МСТПР241.

## Подготовка окружения

Как и было предложено, работа с проектом происходит посредством развертывания отдельного venv-окружения:

```shell
python3 -m venv venv
```

Датасет ToxiCR подгружается с помощью Git-сабмодуля:

```shell
git submodule update --init
```

## Работа с классическими методами ML

В первую очередь хотелось рассмотреть, как себя будут показывать простейшие алгоритмы с гиперпараметрами по умолчанию.
На тот момент никакой хитрой обработки входных данных не было: лишь отсечения пустых значений (`None`) и удаление дубликатов.

Основной интерес представляла логистическая регрессия из `sklearn` на основе tf-idf или bag-of-words - сначала был рассмотрен tf-idf.
По умолчанию логистическая регрессия из `sklearn` использует солвер `lbfgs` с ridge-регуляризацией (l2) и обратным коэффициентом 1.
Для такой конфигурации были получены следующие результаты:

```
Mean cross-entropy loss: 4.27361978342756
True non-toxic comments recognized: 10246
Non-toxic comments treated as toxic: 136
Toxic comments treated as non-toxic: 1394
True toxic comments recognized: 1128
Final F1-score: 0.595879556259905
```

Далее я вспомнил, что lasso-регуляризация (l1) может быть более эффективной в случае, когда у нас имеется большое количество признаков - для tf-idf и bag-of-words это вполне ожидаемо.

Заменив l2 на l1 и солвер `lbfgs` на `liblinear` (первый не работает с l1) я получил следующие результаты:

```
Mean cross-entropy loss: 3.1200217634565917
True non-toxic comments recognized: 10192
Non-toxic comments treated as toxic: 190
Toxic comments treated as non-toxic: 927
True toxic comments recognized: 1595
Final F1-score: 0.7406547480845136
```

Я пришел к выводу, что теперь стоило бы попробовать другую форму извлечения признаков - обычный bag-of-words вместо tf-idf.
Идействительно - показатели F1-метрики увеличились:

```
Для kfold=10, count, l1, liblinear:
Mean cross-entropy loss: 2.938462753049538
True non-toxic comments recognized: 10051
Non-toxic comments treated as toxic: 331
Toxic comments treated as non-toxic: 721
True toxic comments recognized: 1801
Final F1-score: 0.7739578856897292
```

Немного модифицировав обратный коэффициент регуляризации - заменив 1 на 1.35 - я еще больше увеличил F1-метрику:

```
Mean cross-entropy loss: 2.9049441665128515
True non-toxic comments recognized: 10022
Non-toxic comments treated as toxic: 360
Toxic comments treated as non-toxic: 680
True toxic comments recognized: 1842
Final F1-score: 0.7798475867908552
```

## Работа с трансформером CodeBERT



### Описание задания

Цель данного задания - разработать программное решение на языке Python для
классификации рецензий исходного кода.

Примеры токсичных комментариев:

```
 - doh. its awful! it should not be our work...
 - Chris, just a question about 'intrusive', how can I understand it? Why it sucks.
```

Примеры не токсичных комментариев:
```
 - Please, remove indention of commit message.
 - Here should be admin_snapshots_client.reset_snapshot_status
```

### Подзадача 1: Подготовка набора данных

#### Описание

**Цель**: Подготовка набора данных, содержащего размеченные рецензии исходного
кода из репозитория [**ToxiCR**](https://github.com/WSU-SEAL/ToxiCR/tree/master).

#### Шаги выполнения

1. Проведите очистку набора данных, устраняя пропущенные значения и дубликаты.
2. Подготовьте текстовые данные для анализа, выполнив следующие пункты
(опционально, вдохновлено оригинальной работой авторов репозитория):
 - Удаление URL-ссылок, например ссылок на документацию, посты Stackoverflow
 - Исправление сокращиений слов, например doesn’t -> does not и we’re -> we are
 для унификации токенов. (готовый словарь можно найти в репозитории ToxiCR)
 - Удаление повторяющихся символов: “You’re duumbbbb!,” -> “you are dumb”
 - Удаление специальных символов &, #, ^, *...
 - Исправление специально испорченных ругательных слов (словарь с регулярными
 выражениями также можно найти в репозитории ToxiCR)
 - Ваши предложения по очистке датасета...
3. После предобработки сохраните подготовленные данные для дальнейшего
использования

### Подзадача 2: Использование моделей машинного обучения для классификации комментариев

**Цель**: Попробовать набор различных подходов к классификации текстов.

**Модели**:

 - Классические модели (Logistic Regression, Random Forest)
 - [**RoBERTa**](https://huggingface.co/FacebookAI/roberta-base)
 - [**CodeBERT**](https://huggingface.co/microsoft/codebert-base):
    предобученная модель трансформер семейства BERT, адаптированная для
    работы с исходным кодом.

#### Шаги выполнения

##### Использование классических методов машинного обучения (библиотека [**scikit-learn**](https://scikit-learn.org/stable/supervised_learning.html))

1. Для преобразования в числовое представление ранее подготовленных текстов
используйте CountVectorizer и/или TfidfVectorizer.
2. Обучите модели Random Forest и/или Logistic Regression на полученных
CountVectorizer и/или TfidfVectorizer.
3. Проведите оценку качества моделей путем 10-фолдовой кросс-валидации (KFold cross-validation)
4. Постройте и проанализируйте матрицу несоответствия (confusion matrix).
5. Попробуйте улучшить качество классификации путем экспериментов с
гипер-параметрами моделей и методов для извлечения признаков.

##### Использование предобученных моделей (библиотека [**transformers**](https://huggingface.co/docs/transformers/tasks/sequence_classification))

1. Используйте токенизатор RoBERTa (CodeBERT) для преобразования текстовых
данных в формат, понятный модели.
2. Инициализируйте модель RoBERTa/CodeBERT
(`AutoModelForSequenceClassification`) и объект Trainer, который будет
управлять процессом обучения. Определите параметры обучения, такие как
количество эпох, размер батча и скорость обучения. Запустите процесс обучения
модели на обучающих данных.
3. Оцените качество модели метриками `accuracy`, `precision_recall_fscore_support`,
добавив в Trainer параметр `compute_metrics`, реализовав соответствующие метрики.
4. Подготовьте краткий (1-2 стр.) отчет, который будет включать сравнение по
метрикам accuracy, precision, recall, f1-score всех реализованных моделей на
отложенном eval наборе данных.


### Сроки выполнения

Общий срок выполнения задания: 2 недели

### Дополнительные указания

- При выполнения задания создайте репозиторий на GitHub, в котором будет находиться текущий шаблон и весь код реализации
- Решение может быть выполнено как в виде модулей на языке Python так и в виде Jupyter Notebook'ов.
- Используйте виртуальное окружение для установки всех зависимостей, большинство зависимостей уже зафиксировано в шаблоне
- В отчете опишите все проблемы, с которыми вы столкнулись, и как вы их решили.
